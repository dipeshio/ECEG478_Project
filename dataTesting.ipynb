{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath: vscode-notebook-cell:/c%3A/Users/User/Documents/Projects/ECEG478_Project/dataTesting.ipynb#W0sZmlsZQ%3D%3D\n",
    "%pip install seaborn\n",
    "%pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T18:29:35.304754Z",
     "start_time": "2025-04-22T18:29:31.513632Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## loading in all of the important libraries\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b535f7b869d6fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T19:51:41.481706Z",
     "start_time": "2025-04-22T19:51:41.186440Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading in the two datasets (training and eval) and getting some metrics out of it\n",
    "df = pd.read_csv('FairFace/fairface_label_train.csv')\n",
    "df2 = pd.read_csv('FairFace/fairface_label_val.csv')\n",
    "# remove service_test column\n",
    "df = df.drop(columns=['service_test'])\n",
    "df2 = df2.drop(columns=['service_test'])\n",
    "\n",
    "## Combining the age bins of '60-69' and 'more than 70' into '60+' and relabeling the age bins\n",
    "df['age'] = df['age'].replace({'60-69': '60+', 'more than 70': '60+'})\n",
    "df2['age'] = df2['age'].replace({'60-69': '60+', 'more than 70': '60+'})\n",
    "\n",
    "# rename Latino_Hispanic to Latino\n",
    "df['race'] = df['race'].replace({'Latino_Hispanic': 'Latino'})\n",
    "df2['race'] = df2['race'].replace({'Latino_Hispanic': 'Latino'})\n",
    "\n",
    "# confirmation check\n",
    "# print(\"Value counts AFTER replacement, BEFORE filtering:\")\n",
    "# print(df['age'].value_counts())\n",
    "# print(f\"Total rows before filtering: {len(df)}\")\n",
    "\n",
    "df = df.drop_duplicates(subset=['file'], keep='first')\n",
    "df2 = df2.drop_duplicates(subset=['file'], keep='first')\n",
    "\n",
    "\n",
    "\n",
    "# encoding categorical labels\n",
    "age_bins = ['0-2', '3-9', '10-19', '20-29', '30-39', '40-49', '50-59', '60+']\n",
    "\n",
    "# new column 'age' with the bins from above\n",
    "df = df[df['age'].isin(age_bins)]  # ensure only those bins are used\n",
    "df2 = df2[df2['age'].isin(age_bins)] \n",
    "\n",
    "\n",
    "# confirmation check post transform\n",
    "# print(\"\\nValue counts AFTER filtering:\")\n",
    "# print(df['age'].value_counts())\n",
    "# print(f\"Total rows after filtering: {len(df)}\")\n",
    "\n",
    "\n",
    "## these will be used to check what the encoding and decoding looks like\n",
    "age_encoder = LabelEncoder()\n",
    "df['age_label'] = age_encoder.fit_transform(df['age'])\n",
    "df2['age_label'] = age_encoder.fit_transform(df2['age'])\n",
    "\n",
    "\n",
    "gender_encoder = LabelEncoder()\n",
    "df['gender_label'] = gender_encoder.fit_transform(df['gender'])\n",
    "df2['gender_label'] = gender_encoder.fit_transform(df2['gender'])\n",
    "\n",
    "\n",
    "race_encoder = LabelEncoder()\n",
    "df['race_label'] = race_encoder.fit_transform(df['race'])\n",
    "df2['race_label'] = race_encoder.fit_transform(df2['race'])\n",
    "\n",
    "\n",
    "\n",
    "## generate random int in general\n",
    "def random_int(min_val, max_val):\n",
    "    # function that generates a random int between two input ranges\n",
    "    return np.random.randint(min_val, max_val + 1)\n",
    "\n",
    "def balance_by_multiple_attributes(df, attr_cols, n_per_group):\n",
    "    # function that balances the dataset by multiple attributes\n",
    "    \n",
    "    state_value = random_int(0, 140) ## for creating a random-random state\n",
    "    ## I saw that using a static random state value of 0 was causing the same random sample to be generated every time\n",
    "    \n",
    "    grouped = df.groupby(attr_cols)\n",
    "    balanced_df = grouped.apply(lambda x: x.sample(n=min(len(x), n_per_group), random_state=state_value))\n",
    "    balanced_df = balanced_df.reset_index(drop=True)\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1bf092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: I think creating a new set to sample from would be better than sampling from the original set\n",
    "## This is not being used right now, but this can be used as a reference for the future\n",
    "\n",
    "### These would be the larger sets (still smaller than the original set) that we can sample from\n",
    "# trainSet = balance_by_multiple_attributes(df, ['age_label', 'gender_label', 'race_label'], 500)\n",
    "# trainSet = trainSet.sample(frac=1, random_state=random_int(0,1500)).reset_index(drop=True)\n",
    "\n",
    "valSet = balance_by_multiple_attributes(df2, ['age_label', 'gender_label', 'race_label'], 100)\n",
    "valSet = valSet.sample(frac=1, random_state=random_int(0,1500)).reset_index(drop=True)\n",
    "\n",
    "# ===================================================================================\n",
    "\n",
    "\n",
    "trainingSet = balance_by_multiple_attributes(df, ['age_label', 'gender_label', 'race_label'], 250)\n",
    "## remove age and gender columns\n",
    "trainingSet = trainingSet.drop(['age', 'gender','race'], axis=1)\n",
    "\n",
    "## remove indices that are in the training column so that they don't end up in the validation set\n",
    "remaining = df[~df['file'].isin(trainingSet['file'])]\n",
    "\n",
    "validationSet = balance_by_multiple_attributes(remaining, ['age_label', 'gender_label', 'race_label'], 25)\n",
    "## remove\n",
    "validationSet = validationSet.drop(['age', 'gender','race'], axis=1)\n",
    "\n",
    "## just double checking that there are no overlap between the two sets\n",
    "# check the file name of the training and validation sets via if loop (using tqdm?)\n",
    "training_files = set(trainingSet['file'])\n",
    "validation_files = set(validationSet['file'])\n",
    "\n",
    "# print(training_files)\n",
    "# print(validation_files)\n",
    "\n",
    "# print(f\"overlap b/w training and validation sets: {overlap}\")\n",
    "# print(f\"size of overlap: {len(overlap)}\")\n",
    "\n",
    "\n",
    "# print(newSet.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a0a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation set output\n",
    "name = valSet\n",
    "## Age labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(name['age_label'].value_counts().index, name['age_label'].value_counts().values)\n",
    "plt.title('Age Distribution')\n",
    "plt.xlabel('Age Label')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "## count how many are label 7 and 8\n",
    "print(name['age_label'].value_counts())\n",
    "# 3270 as a combination of 7 and 8 before combinations\n",
    "# \n",
    "\n",
    "## Gender labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(name['gender_label'].value_counts().index, name['gender_label'].value_counts().values)\n",
    "plt.title('Gender Distribution')\n",
    "plt.xlabel('Gender Label')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "## Race labels\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(name['race_label'].value_counts().index, name['race_label'].value_counts().values)\n",
    "plt.title('Race Distribution')\n",
    "plt.xlabel('Race Label')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddddefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## export the training and validation sets to csv files\n",
    "\n",
    "# before randomizing\n",
    "print(\"Before randomizing:\")\n",
    "print(trainingSet.head())\n",
    "print(validationSet.head())\n",
    "print(\"================================================================\")\n",
    "\n",
    "# randomize the training and validation sets\n",
    "trainingSet = trainingSet.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "validationSet = validationSet.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# after randomizing\n",
    "print(trainingSet.head())\n",
    "print(validationSet.head())\n",
    "\n",
    "# create a new direcotry called Datasets\n",
    "if not os.path.exists('Datasets'):\n",
    "    os.makedirs('Datasets')\n",
    "\n",
    "trainingSet.to_csv('Datasets/trainingSet.csv', index=False)\n",
    "validationSet.to_csv('Datasets/validationSet.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f7b4d7809f3ff4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-22T19:23:11.553965Z",
     "start_time": "2025-04-22T19:23:11.524495Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "## Output of Encoding for age and race\n",
    "# tester111 = (list(zip(age_encoder.classes_, range(len(age_encoder.classes_)))))  ## output is in format of [(class, index), ...]\n",
    "tester111 = (list(zip(gender_encoder.classes_, range(len(gender_encoder.classes_)))))\n",
    "\n",
    "## need to change to [class, class, class, ...]\n",
    "for i in range(len(tester111)):\n",
    "    tester111[i] = tester111[i][0]\n",
    "print(tester111)\n",
    "    \n",
    "# print(list(zip(gender_encoder.classes_, range(len(gender_encoder.classes_)))))\n",
    "# print(list(zip(race_encoder.classes_, range(len(race_encoder.classes_)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304aa3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL   import Image\n",
    "from pathlib import Path\n",
    "\n",
    "# ── transforms ──────────────────────────────────────────────\n",
    "IMG_SIZE = 224\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],\n",
    "                         [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],\n",
    "                         [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "# ── dataset ─────────────────────────────────────────────────\n",
    "class FairFaceMulti(Dataset):\n",
    "    def __init__(self, df, img_dir, train=True):\n",
    "        self.df       = df.reset_index(drop=True)\n",
    "        self.img_dir  = Path(img_dir)\n",
    "        self.tfm      = train_tfms if train else val_tfms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row   = self.df.iloc[idx]\n",
    "        img_p = self.img_dir / Path(row.file).name   # robust join\n",
    "        img   = Image.open(img_p).convert(\"RGB\")\n",
    "        age    = torch.tensor(row.age_label,    dtype=torch.long)\n",
    "        gender = torch.tensor(row.gender_label, dtype=torch.long)\n",
    "        race = torch.tensor(row.race_label, dtype=torch.long)\n",
    "\n",
    "        return self.tfm(img), {\"age\": age, \"gender\": gender, \"race\": race}\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"FairFaceMulti(n={len(self)}, dir={self.img_dir})\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15ca26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class AgeGenderNet(nn.Module):\n",
    "    def __init__(self, n_age=9, n_gender=2, n_race=7, backbone=\"resnet18\"):\n",
    "        super().__init__()\n",
    "        self.backbone = getattr(models, backbone)(weights=\"IMAGENET1K_V1\")\n",
    "        dim = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()          # remove final FC\n",
    "\n",
    "        self.age_head    = nn.Linear(dim, n_age)      # 9 classes\n",
    "        self.gender_head = nn.Linear(dim, n_gender)   # 2 classes\n",
    "        self.race_head = nn.Linear(dim, n_race)   # 2 classes\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.backbone(x)\n",
    "        return {\n",
    "            \"age\":    self.age_head(feat),\n",
    "            \"gender\": self.gender_head(feat),\n",
    "            \"race\": self.race_head(feat)\n",
    "\n",
    "        }\n",
    "\n",
    "model = AgeGenderNet().cuda()\n",
    "loss_age    = nn.CrossEntropyLoss()\n",
    "loss_gender = nn.CrossEntropyLoss()     # fairly balanced already\n",
    "loss_race = nn.CrossEntropyLoss()     # fairly balanced already\n",
    "\n",
    "\n",
    "def criterion(pred_dict, target_dict, λ_age=1.0, λ_gender=1.0, λ_race=1.0):\n",
    "    L_age    = loss_age(pred_dict[\"age\"],    target_dict[\"age\"])\n",
    "    L_gender = loss_gender(pred_dict[\"gender\"], target_dict[\"gender\"])\n",
    "    L_race = loss_race(pred_dict[\"race\"], target_dict[\"race\"])\n",
    "\n",
    "    \n",
    "    return λ_age*L_age + λ_gender*L_gender + λ_race*L_race , {\"age\": L_age.item(),\n",
    "                                            \"gender\": L_gender.item(), \"race\": L_race.item()}\n",
    "    \n",
    "    \n",
    "## optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e4c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check nvidia-smi\n",
    "nvidia_smi = os.popen(\"nvidia-smi\").read()\n",
    "!nvidia-smi\n",
    "# check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da76a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(loader, train=True):\n",
    "    model.train() if train else model.eval()\n",
    "    epoch_loss, correct_age, correct_gender, correct_race, n = 0,0,0,0,0 # Initialize n=0\n",
    "    print(f\"Starting {'training' if train else 'validation'} epoch...\")\n",
    "    batch_num = 0\n",
    "    for imgs, targets in tqdm.tqdm(loader):\n",
    "        # print(f\"Processing batch {batch_num}\")\n",
    "        batch_num += 1\n",
    "        try:\n",
    "            imgs = imgs.cuda()\n",
    "            t_age    = targets[\"age\"].cuda()\n",
    "            t_gender = targets[\"gender\"].cuda()\n",
    "            t_race   = targets[\"race\"].cuda() # Get race target\n",
    "\n",
    "            with torch.set_grad_enabled(train):\n",
    "                preds = model(imgs)\n",
    "                loss, parts = criterion(preds, {\"age\":t_age, \"gender\":t_gender, \"race\": t_race}) # Include race in criterion if needed\n",
    "\n",
    "            if train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            _, pa = preds[\"age\"].max(1)\n",
    "            _, pg = preds[\"gender\"].max(1)\n",
    "            _, pr = preds[\"race\"].max(1) # Use pr for race prediction index\n",
    "\n",
    "            correct_age    += (pa==t_age).sum().item()\n",
    "            correct_gender += (pg==t_gender).sum().item()\n",
    "            correct_race   += (pr==t_race).sum().item() # Calculate correct race predictions\n",
    "\n",
    "            n += imgs.size(0)\n",
    "            epoch_loss += loss.item()*imgs.size(0)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_num-1}: {e}\")\n",
    "            # Decide if you want to stop or continue\n",
    "            # raise # Re-raise the exception to stop execution\n",
    "            # continue # Skip this batch and continue\n",
    "\n",
    "    print(f\"Finished {'training' if train else 'validation'} epoch.\")\n",
    "    # Handle potential division by zero if n remains 0\n",
    "    if n == 0:\n",
    "        print(\"Warning: No items processed in the loader.\")\n",
    "        return (0, 0, 0, 0) # Return 4 zeros\n",
    "    # Return loss and all three accuracies\n",
    "    return (epoch_loss/n,\n",
    "            correct_age/n,\n",
    "            correct_gender/n,\n",
    "            correct_race/n) # Add race accuracy to the return tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e992119b",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## just recalling the variables\n",
    "trainingSet = trainingSet\n",
    "validationSet = validationSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f04f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, datetime\n",
    "from statistics import mean\n",
    "\n",
    "## the actual training\n",
    "train_ds = FairFaceMulti(trainingSet,   \"FairFace/train\", train=True)\n",
    "val_ds   = FairFaceMulti(validationSet, \"FairFace/train\", train=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,  num_workers=0, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    \n",
    "# Check the length of the dataset\n",
    "print(f\"Length of train_loader: {len(train_loader)}\")\n",
    "print(f\"Length of val_loader: {len(val_loader)}\")\n",
    "\n",
    "## location to store models\n",
    "if not os.path.exists('checkpoints'):\n",
    "    os.makedirs('checkpoints')\n",
    "\n",
    "EPOCHS = 200\n",
    "# to keep track of time\n",
    "epoch_times = [] \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    tr_loss, tr_acc_age, tr_acc_gen, tr_acc_race = run_epoch(train_loader, train=True)\n",
    "    vl_loss, vl_acc_age, vl_acc_gen, vl_acc_race = run_epoch(val_loader,   train=True)\n",
    "    scheduler.step()\n",
    "    # Initialize variables for ETA calculation\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Calculate elapsed time and ETA\n",
    "    epoch_times.append(time.time() - t0)\n",
    "    avg_epoch   = mean(epoch_times)\n",
    "    remaining   = EPOCHS - (epoch + 1)\n",
    "    eta_seconds = avg_epoch * remaining\n",
    "    eta_str     = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "\n",
    "    # Format ETA as HH:MM:SS\n",
    "    eta_formatted = time.strftime(\"%H:%M:%S\", time.gmtime(eta))\n",
    "    \n",
    "    ## incramentally save the model after every 20 epochs\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        torch.save(model.state_dict(), f\"checkpoints/model_epoch_{epoch+1}.pth\")\n",
    "        print(f\"Model saved at epoch {epoch+1}\")\n",
    "\n",
    "    # Print metrics\n",
    "    print(\n",
    "        f\"{epoch:02d}  \"\n",
    "        f\"tr_loss={tr_loss:.3f}  vl_loss={vl_loss:.3f} |\\n\"\n",
    "        f\"TRAIN  age={tr_acc_age:.2%}  gen={tr_acc_gen:.2%}  race={tr_acc_race:.2%} |\\n\"\n",
    "        f\"VAL    age={vl_acc_age:.2%}  gen={vl_acc_gen:.2%}  race={vl_acc_race:.2%} |\\n\"\n",
    "        f\"ETA ~ {eta_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c14fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the model after training\n",
    "\n",
    "ckpt_path = \"checkpoints/best_multitask.pth\"\n",
    "ckpt = {\n",
    "    \"epoch\"      : epoch,\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"optimizer\"  : optimizer.state_dict(),   # optional\n",
    "    \"age_encoder\": age_encoder.classes_.tolist(),  # any extra objects you need\n",
    "}\n",
    "torch.save(ckpt, ckpt_path)\n",
    "print(\"✅  Saved checkpoint →\", ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a057d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # images need to be randomly selected\n",
    "# import random\n",
    "\n",
    "# # loading in the dataset and getting some metrics out of it\n",
    "# testSet = pd.read_csv('FairFace/fairface_label_val.csv')\n",
    "\n",
    "# # remove service_test column\n",
    "# testSet = testSet.drop(columns=['service_test'])\n",
    "# ## Combining the age bins of '60-69' and 'more than 70' into '60+' and relabeling the age bins\n",
    "# testSet['age'] = testSet['age'].replace({'60-69': '60+', 'more than 70': '60+'})\n",
    "# # rename Latino_Hispanic to Latino\n",
    "# testSet['race'] = testSet['race'].replace({'Latino_Hispanic': 'Latino'})\n",
    "\n",
    "# testSet = testSet.drop_duplicates(subset=['file'], keep='first')\n",
    "\n",
    "\n",
    "# # encoding categorical labels\n",
    "# age_bins = ['0-2', '3-9', '10-19', '20-29', '30-39', '40-49', '50-59', '60+']\n",
    "\n",
    "# # new column 'age' with the bins from above\n",
    "# testSet = testSet[testSet['age'].isin(age_bins)]  # ensure only those bins are used\n",
    "\n",
    "# ## these will be used to check what the encoding and decoding looks like\n",
    "# age_encoder = LabelEncoder()\n",
    "# testSet['age_label'] = age_encoder.fit_transform(testSet['age'])\n",
    "\n",
    "# gender_encoder = LabelEncoder()\n",
    "# testSet['gender_label'] = gender_encoder.fit_transform(testSet['gender'])\n",
    "\n",
    "# race_encoder = LabelEncoder()\n",
    "# testSet['race_label'] = race_encoder.fit_transform(testSet['race'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041a74fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import shutil\n",
    "\n",
    "# --- Make sure valSet is defined correctly from your validation data ---\n",
    "# Example: valSet = validationSet # Assuming validationSet is your DataFrame\n",
    "\n",
    "## Need to take around 25-30 image from the val folder in FairFace and put them in a folder called test\n",
    "# define the two source base directory and the destination directory\n",
    "source_base_dir = \"FairFace\" ## where it's coming from\n",
    "test_dir = \"FairFace/test\"   ## where it'll go\n",
    "\n",
    "if not os.path.exists(test_dir):\n",
    "    os.makedirs(test_dir)\n",
    "    \n",
    "## check what files are in the directory and remove all\n",
    "for filename in os.listdir(test_dir):\n",
    "    file_path = os.path.join(test_dir, filename)\n",
    "    try:\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "            # print(f\"Removed {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error removing {file_path}: {e}\")\n",
    "\n",
    "# get a list of all the files (relative paths like 'val/xxxx.jpg') in the validation set\n",
    "testFiles = valSet['file'].tolist()\n",
    "\n",
    "# this randomly select 25 files\n",
    "if len(testFiles) >= 25:\n",
    "    random_files = random.sample(testFiles, 25)\n",
    "    print(random_files)\n",
    "    print(len(random_files))\n",
    "else:\n",
    "    print(f\"Warning: Only {len(testFiles)} files available in validation set. Selecting all.\")\n",
    "    random_files = testFiles\n",
    "\n",
    "print(f\"Attempting to copy {len(random_files)} files to {test_dir}...\")\n",
    "\n",
    "# Copy the files to the test directory\n",
    "for file_relative_path in random_files:\n",
    "    # source path\n",
    "    src = os.path.join(source_base_dir, file_relative_path)\n",
    "    # file name\n",
    "    file_basename = os.path.basename(file_relative_path)\n",
    "    # dest pat\n",
    "    dst = os.path.join(test_dir, file_basename)\n",
    "    \n",
    "    # loop through the files and copy them to the test directory\n",
    "    try:\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy2(src, dst) # Use shutil.copy2 for better copying\n",
    "            # print(f\"Copied {src} to {dst}\")\n",
    "        else:\n",
    "            print(f\"Error: Source file {src} does not exist.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error copying {src} to {dst}: {e}\")\n",
    "\n",
    "print(\"Finished copying files.\")\n",
    "\n",
    "## Now need to create a new CSV file for all of the information regarding the files in the test directory\n",
    "# gen idea is to go call the val csv file and drop all rows that do not have the file name in the test directory\n",
    "\n",
    "files = os.listdir(test_dir)\n",
    "# Get the file names without the dir path\n",
    "files = [os.path.basename(file) for file in files]\n",
    "\n",
    "# new of val labels\n",
    "## file column contains file names in the style of val/xxxx.jpg\n",
    "# filter df2 to only include rows where the 'file' column matches any of the file names in the test directory\n",
    "# Extract the basename from the 'file' column in df2\n",
    "df2_basenames = df2['file'].str.split('/').str[-1]\n",
    "\n",
    "# Filter df2 where the basename is in the 'files' list\n",
    "testertester = df2[df2_basenames.isin(files)].copy() # Use .copy() to avoid some err\n",
    "# testertester.drop(columns=['age_label', 'gender_label', 'race_label'], inplace=True)\n",
    "# print(testertester)\n",
    "\n",
    "## save as csv in the file path\n",
    "testertester.to_csv('FairFace/fairface_label_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "ac908a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_41780\\1567078367.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(r\"checkpoints\\model_epoch_200.pth\"))\n"
     ]
    }
   ],
   "source": [
    "## Testing the model\n",
    "## select the model\n",
    "model = AgeGenderNet()\n",
    "model.load_state_dict(torch.load(r\"checkpoints\\model_epoch_200.pth\"))\n",
    "model.eval()                      \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "@torch.inference_mode()                  # no gradients, saves memory\n",
    "def predict_img(img_path: str):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x   = val_tfms(img).unsqueeze(0).to(device)     # shape (1,3,224,224)\n",
    "\n",
    "    out = model(x)                       # tuple OR dict, depending on your net\n",
    "    age_id    = out[\"age\"].argmax(1).item()\n",
    "    gender_id = out[\"gender\"].argmax(1).item()\n",
    "    race_id   = out[\"race\"].argmax(1).item()\n",
    "    \n",
    "    # print(\"ID INFO\")\n",
    "    # print(f\"Age ID: {age_id}, Gender ID: {gender_id}, Race ID: {race_id}\")\n",
    "\n",
    "    # class-name look-up tables (same order you used to encode)\n",
    "    age_classes = (list(zip(age_encoder.classes_, range(len(age_encoder.classes_)))))  ## output is in format of [(class, index), ...]\n",
    "    gender_map = (list(zip(gender_encoder.classes_, range(len(gender_encoder.classes_)))))\n",
    "    race_classes = (list(zip(race_encoder.classes_, range(len(race_encoder.classes_)))))\n",
    "    \n",
    "    ## need to change to [class, class, class, ...]\n",
    "    for i in range(len(age_classes)):\n",
    "        age_classes[i] = age_classes[i][0]\n",
    "    # print(age_classes)\n",
    "    \n",
    "    ## repeat for the other two\n",
    "    for i in range(len(gender_map)):\n",
    "        gender_map[i] = gender_map[i][0]\n",
    "    # print(gender_map)\n",
    "    \n",
    "    for i in range(len(race_classes)):\n",
    "        race_classes[i] = race_classes[i][0]\n",
    "    # print(race_classes)\n",
    "            \n",
    "    return age_classes[age_id], gender_map[gender_id], race_classes[race_id]\n",
    "\n",
    "\n",
    "# img_dir = r\"FairFace/test/2548.jpg\"\n",
    "# result = predict_img(img_dir)   # any face image\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b27d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "# load in testing csv set\n",
    "valFilePath = \"FairFace/fairface_label_test.csv\"\n",
    "valSet = pd.read_csv(valFilePath)\n",
    "\n",
    "test_file_path = r\"FairFace\\test\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## test with batches\n",
    "def model_test_batch(test_path):\n",
    "    # get all files in the test directory, in format: FairFace/test/xxxx.jpg\n",
    "    test_files = os.listdir(test_path)\n",
    "    test_files = [os.path.join(test_path, file) for file in test_files]\n",
    "    \n",
    "    # extract values of age_range, gender, and race from the valSet and save it to a list for accuracy comparison later\n",
    "    # this should only already contain the files that are in the test directory\n",
    "    # age list should be: [3-9, 10-19, 20-29, 30-39, 40-49, 50-59, 60+, ...] for each image\n",
    "    \n",
    "    true_age_list = valSet['age'].tolist()\n",
    "    true_gender_list = valSet['gender'].tolist()\n",
    "    true_race_list = valSet['race'].tolist()\n",
    "    \n",
    "    # empty lists to store the predictions\n",
    "    pred_age_list = []\n",
    "    pred_gender_list = []\n",
    "    pred_race_list = []\n",
    "    \n",
    "    # loop through the test files and get the predictions\n",
    "    for file in test_files:\n",
    "        pred_age, pred_gender, pred_race = predict_img(file)\n",
    "        pred_age_list.append(pred_age), pred_gender_list.append(pred_gender), pred_race_list.append(pred_race)\n",
    "    \n",
    "    # print accuracies\n",
    "    print(f\"Age accuracy: {accuracy_score(true_age_list, pred_age_list)}/n\n",
    "          Gender Accuracy: {accuracy_score(true_gender_list, pred_gender_list)}/n\n",
    "          Race Accuracy: {accuracy_score(true_race_list, pred_race_list)}\")\n",
    "        \n",
    "    \n",
    "        \n",
    "model_test_batch(test_file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4f41bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e32c6cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
